---
#
# edX Configuration
#
# github:     https://github.com/edx/configuration
# wiki:       https://openedx.atlassian.net/wiki/display/OpenOPS
# code style: https://openedx.atlassian.net/wiki/display/OpenOPS/Ansible+Code+Conventions
# license:    https://github.com/edx/configuration/blob/master/LICENSE.TXT
#
##
# Defaults for role hadoop_common
#

HADOOP_COMMON_VERSION: 2.7.2
HADOOP_COMMON_USER_HOME: "{{ COMMON_APP_DIR }}/hadoop"
HADOOP_COMMON_HOME: "{{ HADOOP_COMMON_USER_HOME }}/hadoop"
HADOOP_COMMON_DATA: "{{ COMMON_DATA_DIR }}/hadoop"

# These are non-standard directories, but are where Hadoop expects to find them.
HADOOP_COMMON_LOGS: "{{ HADOOP_COMMON_HOME }}/logs"
HADOOP_COMMON_CONF_DIR: "{{ HADOOP_COMMON_HOME }}/etc/hadoop"

HADOOP_COMMON_PROTOBUF_VERSION: 2.5.0
HADOOP_COMMON_SERVICES_DIR: "{{ HADOOP_COMMON_USER_HOME }}/services.d"
HADOOP_COMMON_SERVICE_HEAP_MAX: 256
HADOOP_COMMON_TOOL_HEAP_MAX: 128

hadoop_common_role_name: hadoop_common
hadoop_common_user: hadoop
hadoop_common_group: hadoop
hadoop_common_temporary_dir: /var/tmp
hadoop_common_dist:
  filename: "hadoop-{{ HADOOP_COMMON_VERSION }}.tar.gz"
  url: "https://archive.apache.org/dist/hadoop/common/hadoop-{{ HADOOP_COMMON_VERSION }}/hadoop-{{ HADOOP_COMMON_VERSION }}.tar.gz"
  sha256sum: 49AD740F85D27FA39E744EB9E3B1D9442AE63D62720F0AABDAE7AA9A718B03F7

hadoop_common_protobuf_dist:
  filename: "protobuf-{{ HADOOP_COMMON_PROTOBUF_VERSION }}.tar.gz"
  url: "https://github.com/google/protobuf/releases/download/v{{ HADOOP_COMMON_PROTOBUF_VERSION }}/protobuf-{{ HADOOP_COMMON_PROTOBUF_VERSION }}.tar.gz"
  sha256sum: c55aa3dc538e6fd5eaf732f4eb6b98bdcb7cedb5b91d3b5bdcf29c98c293f58e

hadoop_common_native_dist:
  filename: "hadoop-{{ HADOOP_COMMON_VERSION }}-src.tar.gz"
  url: "https://archive.apache.org/dist/hadoop/common/hadoop-{{ HADOOP_COMMON_VERSION }}/hadoop-{{ HADOOP_COMMON_VERSION }}-src.tar.gz"
  sha256sum: 7d48e61b5464a76543fecf5655d06215cf8674d248b28bc74f613bc8aa047d33

hadoop_common_java_home: "{{ oraclejdk_link }}"
hadoop_common_env: "{{ HADOOP_COMMON_HOME }}/hadoop_env"

#
# OS packages
#
hadoop_common_debian_pkgs:
  - gcc
  - build-essential
  - make
  - cmake
  - automake
  - autoconf
  - libtool
  - zlib1g-dev
  - maven

hadoop_common_redhat_pkgs: []

#
# Vars are used to fill in the following files:
#     core-site.xml
#     hdfs-site.xml
#     mapred-site.xml
#     yarn-site.xml
#
MAPRED_SITE_DEFAULT_CONFIG:
  mapreduce.framework.name: "yarn"

YARN_SITE_DEFAULT_CONFIG:
  yarn.nodemanager.aux-services: "mapreduce_shuffle"
  yarn.nodemanager.aux-services.mapreduce.shuffle.class: "org.apache.hadoop.mapred.ShuffleHandler"
  yarn.log-aggregation-enable: "true"
  # 24 hour log retention
  yarn.log-aggregation.retain-seconds: 86400
  # Checking virtual memory usage causes too many spurious failures.
  yarn.nodemanager.vmem-check-enabled: false

HADOOP_CORE_SITE_DEFAULT_CONFIG:
  fs.default.name: "hdfs://localhost:9000"

HDFS_SITE_DEFAULT_CONFIG:
  dfs.replication: "1"
  dfs.namenode.name.dir: "file:{{ HADOOP_COMMON_DATA }}/namenode"
  dfs.datanode.data.dir: "file:{{ HADOOP_COMMON_DATA }}/datanode"

#
# MapReduce/Yarn memory config (defaults for m1.medium)
# http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/TaskConfiguration_H2.html
#
# mapred_site_config:
#   mapreduce.map.memory_mb: 768
#   mapreduce.map.java.opts: '-Xmx512M'
#   mapreduce.reduce.memory.mb: 1024
#   mapreduce.reduce.java.opts: '-Xmx768M'

# yarn_site_config:
#   yarn.app.mapreduce.am.resource.mb: 1024
#   yarn.scheduler.minimum-allocation-mb: 32
#   yarn.scheduler.maximum-allocation-mb: 2048
#   yarn.nodemanager.resource.memory-mb: 2048
#   yarn.nodemanager.vmem-pmem-ratio: 2.1

#
# Variables override the stock configuration for entry into
# the following files. Ensure that you use unambiguous
# string literals to avoid any confusion:
#     core-site.xml
#     hdfs-site.xml
#     mapred-site.xml
#     yarn-site.xml
#
mapred_site_config: {}
yarn_site_config: {}
HADOOP_CORE_SITE_EXTRA_CONFIG: {}
HDFS_SITE_EXTRA_CONFIG: {}

# Define all the services here, assuming we have a single-node Hadoop
# cluster.
hadoop_common_services:
  - hdfs-namenode
  - hdfs-datanode
  - yarn-resourcemanager
  - yarn-nodemanager
  - yarn-proxyserver
  - mapreduce-historyserver
